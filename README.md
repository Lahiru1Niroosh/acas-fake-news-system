# ACAS â€“ Fake News Detection & Privacy-Aware Verification System

## ğŸ“Œ Project Overview
ACAS (Automated Credibility Assessment System) is a privacy-first, multi-agent fake news detection platform designed to verify health-related and news-related social media content.  
The system uses a **CrewAI-based agent orchestration framework** combined with **FastAPI** and **React** to analyze tweets containing text, images, or both, while ensuring **user metadata privacy through PII masking**.

The platform demonstrates a **plug-and-play architecture**, allowing independently trained AI models to be integrated seamlessly into the verification pipeline.

---

## ğŸ¯ Problem Addressed
The rapid spread of fake newsâ€”especially in health, political, and public safety domainsâ€”causes misinformation, panic, and loss of trust.

Existing solutions often lack:
- Privacy protection
- Explainable decisions
- Modular AI integration

ACAS addresses these gaps through **privacy-aware processing, explainable AI (XAI), and modular agent orchestration**.

---

## ğŸ§  System Architecture (High-Level)

Frontend (React + Tailwind)
â†“  
FastAPI `/verify` API  
â†“  
CrewAI Pipeline Controller  
â†“  
Privacy Agent (PII Masking) â†’ MongoDB (Raw + Masked Metadata)  
â†“  
Routing Logic  
- Text â†’ Text Agent  
- Image â†’ Image Agent  
- Text + Image â†’ Both Agents  
â†“  
Similarity Agent (Final Gate + XAI)  
â†“  
MongoDB (Agent Scores + Final Verdict)  
â†“  
Frontend Modal (Result + Explanation)

---

## ğŸ§© Individual Component Contribution
**Author:** Sathsara W.H.A.L.N - IT22568632

### ğŸ”¹ Component Title
**Plug-and-Play Agent-Orchestration Framework (ACIF) + Privacy Agent**

### ğŸ”¹ Description
This component ensures that user privacy is preserved before any AI-based analysis and orchestrates multiple AI agents using CrewAI in a strict linear pipeline.

### ğŸ”¹ Key Responsibilities
- Designed and implemented the **Privacy Agent** for PII masking
- Enforced **mandatory execution order** using CrewAI
- Built routing logic for text, image, and multimodal tweets
- Enabled **plug-and-play integration** for externally trained ML models

---

## ğŸ§‘â€ğŸ¤â€ğŸ§‘ CrewAI Agents & Roles

| Agent Name | Role | Responsibility |
|-----------|------|----------------|
| Privacy_Agent | PII Masking Officer | Masks all user metadata before processing |
| Text_Agent | Linguistic Analyst | Analyzes textual credibility |
| Image_Agent | Visual Forensic Expert | Analyzes image credibility |
| Similarity_Agent | Final Decision Gatekeeper & XAI Specialist | Produces final verdict and explanation |

---

## ğŸ” Pipeline Execution Flow (Mandatory)

1. Frontend sends raw tweet + user metadata
2. **Privacy Agent runs first (non-skippable)**
3. Routing logic selects relevant agents:
   - Text only â†’ Text Agent
   - Image only â†’ Image Agent
   - Text + Image â†’ Both agents
4. Outputs passed to Similarity Agent
5. Final credibility score + XAI reason generated
6. All results logged in MongoDB
7. Response returned to frontend

---

## ğŸ—„ï¸ MongoDB Data Storage

Each verification stores:
- Original tweet data
- Raw user metadata (audit purpose)
- Masked user metadata
- Individual agent scores
- Final credibility score
- Explainable AI (XAI) reason

---

## ğŸ”Œ Plug-and-Play Model Integration

External teams can integrate their trained models by adding them to:

backend/models/
- text_model.pkl
- image_model.pkl

Integration points are clearly marked in code:



## ğŸ–¥ï¸ Frontend Features

- Twitter-like live feed
- Replaces 5 fake tweets every 10 seconds
- Supports text-only, image-only, and multimodal tweets
- Verify button triggers backend pipeline
- Modal displays:
  - Credibility label
  - Confidence score
  - XAI explanation
  - Masked user ID

---

## âš™ï¸ Project Dependencies

### Backend
- Python 3.10+
- FastAPI
- CrewAI
- Pydantic
- PyMongo
- Uvicorn

### Frontend
- React (Vite)
- Tailwind CSS
- JavaScript (ES6+)

### Database
- MongoDB Atlas / Local MongoDB

---

## ğŸ“ Repository Structure

acas-fake-news-system/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ db/
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/
â”‚   â””â”€â”€ pages/
â””â”€â”€ README.md

---

## ğŸ”„ Version Control & Collaboration

- Git-based development
- Feature-based commits
- Modular agent architecture
- Designed for collaborative model integration
- Supports branching and merging workflows

---

## ğŸš€ Future Enhancements
- Integration of real trained ML models
- Docker-based deployment
- Real-time social media ingestion
- Advanced analytics dashboards

---

## ğŸ‘¤ Author
**Lahiru Niroshan Sathsara**  
Final Year IT Undergraduate â€“ SLIIT  
Component: Agent Orchestration & Privacy Infrastructure

<br><br><br><br>








## ğŸ§© Individual Component Contribution
**Author:** IT22576248 -K.K.K.G. Kalehewatta  
### Component Overview

### ğŸ”¹ Component Title
**-Text Credibility Assessment**
---

### ğŸ”¹ Description
- This Component is an AI-based Text Credibility Assessment System (Text Credibility & Fact-Checking System) designed to detect fake or misleading news content.The system combines Natural Language Processing (NLP), Machine Learning, Semantic Similarity, and News APIâ€“based verification to evaluate the trustworthiness of textual information.
---

### ğŸ”¹ System Responsibilities & Execution Flow (Combined)
- Dataset Loading  -Loads news or social media text from CSV, JSON, or JSONL datasets
- ext Preprocessing & Normalization  - Cleans raw text by removing URLs, symbols, and noise, and normalizes text to improve model understanding
- Semantic Text Embedding Generation -Converts cleaned text into semantic embeddings using Sentence-BERT (SBERT), preserving contextual meaning and intent of the content
- Trainâ€“Test Data Split -Splits data into training and testing sets
- Machine Learning Model Training -Trains classification models, Learns linguistic and semantic patterns of credible vs fake news
- Model Evaluation -Evaluates performance using Accuracy, Precision, Recall, and F1-Score
- External News Verification -Verifies claims using trusted external sources , Integrates News APIs and health sources (e.g., WHO) for cross-checking
- Trust Score Calculation  -Generates a normalized trust score between 0 and 1
- Credibility Prediction for New Text Supports prediction for unseen or real-time news text, outputs credibility label, confidence score, and trust level
- Result Visualization & Output

---

### âš™ï¸ System & Software Requirements
- Hardware Requirements
- CPU (Intel i5 or higher recommended)
- Minimum 8GB RAM
- GPU (Optional, for faster embedding generation)
- Python 3.11
- Google Colab or Local Python Environment
- Internet connection (for APIs)
- 
---

### ğŸ§ª Technologies, Techniques & Algorithms
- Programming Language
- Python
- Libraries & Frameworks
- Pandas, NumPy
- Scikit-learn
- SentenceTransformers (SBERT)
- Hugging Face Transformers
- Matplotlib, Seaborn

  ---

### ğŸŒ External APIs Used (Verification)
- News Verification APIs
- NewsAPI.org
- GNews API
- WorldNewsAPI

---

### ğŸ” Methodology
- Load and preprocess the dataset
- Clean and normalize text
- Generate sentence embeddings
- Train ML classification model
- Verify claims using News APIs
- Compute trust score
- Visualize results and save outputs
  
---

### ğŸ“Š Evaluation Metrics
- Accuracy
- Precision
- Recall
- F1-Score

---

## ğŸ‘¤ Author
**K.K.K.G.Kalehewatta**  
Final Year IT Undergraduate â€“ SLIIT  
Component: Text Credibility Assessment



---




# ğŸ–¼ï¸ COMPONENT 2  - Image Authenticity Detection 

IT22594990 â€“ Ranasinghe K.D

### 1. Component Overview

This component focuses on Image Authenticity Detection, aiming to determine whether an image shared on social media platforms is AI-generated or human-made (real). With the rapid growth of AI-generated visual content, visual misinformation has become increasingly difficult to detect through manual inspection.

The Image Authenticity Detection module leverages deep learningâ€“based computer vision techniques to analyze visual artifacts, textures, and structural patterns that differentiate synthetic images from real photographs. This component operates as an ImageAgent within the CrewAI multi-agent framework and provides reliable image credibility outputs to the AggregatorAgent for multimodal credibility assessment.

Input: Image from social media
Output: AI-generated or Human-made label with a confidence score

### 2. System Features

- Detection of AI-generated vs Human-made images
- Automated image preprocessing (resizing, normalization)
- Feature extraction using deep learning models
- Supervised learning with labeled datasets
- Confidence score generation (range: 0â€“1)
- Integration with CrewAI multi-agent architecture
- Storage of predictions and metadata in PostgreSQL
- Scalable and real-timeâ€“ready system design

### 3. System Architecture (High-Level)

- Image Ingestion (Dataset / Twitter Images)
- Image Preprocessing
- Feature Extraction using CNN
- Image Classification (AI-generated vs Human-made)
- Confidence Score Generation
- Result Storage (PostgreSQL)
- Output forwarded to the AggregatorAgent

### 4. System & Software Requirements

Software Requirements
- Python 3.10 / 3.11
- Google Colab or Local Python Environment
- Internet connection for dataset access

### 5. Technologies, Techniques & Algorithms
Programming Language
- Python
- Libraries & Frameworks
- PyTorch / TensorFlow
- OpenCV
- NumPy
- Matplotlib
- Scikit-learn
- Deep Learning Models
- Convolutional Neural Networks (CNN)
- Transfer Learning (optional â€“ ResNet / EfficientNet)

### 6. External APIs Used

- Kaggle API â€“ for dataset downloading
(Twitter API planned for future fine-tuning and evaluation)

### 7. Methodology

Download labeled image datasets from Kaggle
Preprocess images (resizing, normalization, augmentation)
Split the dataset into training and testing sets
Train a CNN-based image classification model
Validate the model using unseen test images
Generate prediction labels and confidence scores
Store results for analysis and reporting

### 8. Evaluation Metrics

Model performance is evaluated using the following metrics:
Accuracy
Precision
Recall
F1-Score

These metrics ensure reliable classification between AI-generated and human-made images.

### 9. Dataset
Dataset Used

Real vs Fake AI Image Dataset
ğŸ”— https://www.kaggle.com/datasets/virajinduruwa/real-vs-fakeai-image-dataset

#### Dataset Description

Two classes:
real â†’ Human-made images
fake â†’ AI-generated images

Fully labeled dataset suitable for supervised learning

### 10. Decision Logic

The decision-making process follows this logic:

CNN extracts visual features such as textures, patterns, and artifacts
The model compares extracted features with learned distributions from training data

#### The output layer predicts:

AI-generated or Human-made
A confidence score reflects prediction certainty
Final results are forwarded to the AggregatorAgent for multimodal credibility fusion


<br><br><br><br>
---
# COMPONENT 3  - IT22067074

## Multimodal Contextual Similarity Verification Module

### Component Description
A multimodal module to verify contextual similarity between text and images using external tools and explainable AI principles.


### Project Overview
With the rapid spread of misinformation across digital platforms, verifying the contextual alignment between images and textual claims has become a critical challenge. This project proposes and implements a **Multimodal Contextual Similarity Verification Module** that evaluates whether an image and a given text are semantically and contextually aligned.

The system leverages a **pretrained CLIP (Contrastive Languageâ€“Image Pretraining)** model, integrates **OCR-based text extraction** for images containing embedded text, and applies **cosine similarity scoring** to detect contextual matches or mismatches. The solution is designed to be **scalable, explainable**, and suitable for **misinformation detection and fact-checking scenarios**.

### Objectives
- Verify contextual similarity between image and text inputs  
- Detect semantic mismatches indicative of misinformation  
- Integrate OCR for images containing embedded textual information  
- Support extension to external fact-checking tools (e.g., FactCrescendo)  
- Provide **explainable similarity decisions** rather than black-box predictions  


### System Architecture
The system follows a multimodal pipeline combining image processing, text understanding, and explainable decision logic.

**High-level flow:**




























### Explainable AI (XAI) Perspective
Unlike traditional classification-based models, this system is explicitly designed using **Explainable AI (XAI)** principles.

#### 1. Similarity Score Exposure
Each imageâ€“text pair produces a **numeric cosine similarity score** representing semantic alignment between visual and textual content.

#### 2. Threshold-Based Decision Logic
Decisions are made using a **transparent threshold mechanism**:
- Similarity â‰¥ Threshold â†’ **MATCH**
- Similarity < Threshold â†’ **MISMATCH**

The threshold value is **empirically adjusted** based on experimental observations.

#### 3. OCR Evidence Integration
For images containing embedded text, OCR-extracted content is:
- Explicitly displayed
- Included in similarity computation  
This allows users to understand **why** a decision was made.

#### 4. Interpretability Over Prediction
Instead of producing opaque predictions, the system outputs:
- Similarity score
- OCR-extracted text (if present)
- Final contextual decision  

This design makes the system suitable for **fact-checking, misinformation analysis, and research explainability requirements**.


### Technologies & Tools Used

###$ AI & ML Models
- **CLIP (ViT-B/32)** â€“ Pretrained multimodal imageâ€“text model
- **EasyOCR** â€“ Optical Character Recognition for embedded image text

#### Programming & Libraries
- Python 3  
- PyTorch  
- NumPy  
- Pandas  
- Pillow (PIL)  

#### Platforms
- Google Colab (GPU-enabled)  
- GitHub (Version Control)  


### Datasets
- **Custom Test Images**
  - Used to demonstrate real-world scenarios and explainability.


### Decision Logic
A cosine similarity threshold is applied to determine contextual alignment:
- Similarity â‰¥ Threshold â†’ **MATCH**
- Similarity < Threshold â†’ **MISMATCH**

The threshold is adjustable and tuned based on experimental results to balance sensitivity and robustness.




<br><br><br><br>
---








---
## Technologies Used

- **Python 3.11**
- **Development & Execution Environment** â†’ Google Colab, Jupyter Notebook <br>
- **Machine Learning & NLP** â†’ PyTorch, Sentence-Transformers <br>
- **Data Processing** â†’   Pandas, NumPy <br>
- **scikit-learn** â†’ TF-IDF Vectorizer, Logistic Regression   <br>
- **matplotlib** â†’ Model visualization <br>
- **Web & Data Collection** â†’   Requests, BeautifulSoup <br>
- **Databases** â†’ MongoDB Atlas <br>
- **Deployment & DevOps** â†’  FastAPI, Docker <br>
- **Version Control** â†’  Git & GitHub <br>

---
<br><br>
## Future Improvements

-Extend the system to analyze videos alongside images and text together.<br>
-Extend the system across all social media platform posts, not just Twitter.<br>
-Multilingual Support -Sinhala ğŸ‡±ğŸ‡°, Tamil ğŸ‡±ğŸ‡°, English  ğŸŒUse multilingual transformers (mBERT, XLM-R).<br>
-Browser extensions-Enable users to check credibility instantly.<br>
-Ethical & Bias Mitigation Enhancements- fairness across regions and languages.<br>
-Deploy live on **Hugging Face Spaces**.<br>
---
<br><br>
## License
This project is intended for academic and research purposes.
---
